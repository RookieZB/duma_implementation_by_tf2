{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_mrc_tpu.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"toQJl6Q4xjDp"},"source":["A simple MRC example according to https://arxiv.org/abs/2001.09415.  \n","The data is from https://dataset.org/dream/."]},{"cell_type":"code","metadata":{"id":"WWnNxbk7Zokr"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2cGM54SUCfA"},"source":["!pip install transformers\n","!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBlT22miZU8c","executionInfo":{"status":"ok","timestamp":1617601184611,"user_tz":-480,"elapsed":2619,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["%tensorflow_version 2.x\n","\n","import os\n","import warnings\n","import time\n","import math\n","import json\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","from transformers import TFAlbertModel\n","\n","os.chdir('./drive/My Drive/Python/Research')\n","warnings.filterwarnings('ignore')\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NbYyoo5qaVGU"},"source":["MODEL = 'albert-base-v2'  # model name on huggingface\n","VOCAB = 'models/albert_base_en/30k-clean.vocab'  # download vocab file of albert\n","SPM = 'models/albert_base_en/30k-clean.model'  # download spm file of albert\n","FILEPATH = 'tasks/datasets/dream'  # download dream datasets\n","LOWER = True  # whether to process text in lower case\n","LEFTLEN = 460  # max length of passages\n","RIGHTLEN = 52  # max total length of question-answer pairs\n","NCLASS = 3  # num of options\n","INDIM = 768  # hidden dim of bert model\n","HEAD = 12  # num of co-attention heads\n","SIZE = 64  # dim of a co-attention head\n","KLAYER = 2  # num of co-attention layers\n","BATCH = 16  # batch size\n","EPOCH = 3  # num of epochs\n","LRATE = 1e-5  # learning rate\n","DROP = 0.1  # drop rate of co-attention layers\n","\n","resolver_1 = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://'+os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver_1)\n","tf.tpu.experimental.initialize_tpu_system(resolver_1)\n","strategy_1 = tf.distribute.TPUStrategy(resolver_1)\n","tf.config.list_logical_devices('TPU')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VccO06K4uI7"},"source":["Data processing refers to https://github.com/nlpdata/mrc_bert_baseline.  \n","Also, need to download the source code of ALBERT tokenizer. "]},{"cell_type":"code","metadata":{"id":"nC31zLfUUiLf","executionInfo":{"status":"ok","timestamp":1617601238780,"user_tz":-480,"elapsed":38057,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["from utils.albert import tokenization\n","\n","\n","class InputExample(object):\n","  def __init__(self, guid, text_a, text_b=None, label=None, text_c=None):\n","    self.guid = guid\n","    self.text_a = text_a\n","    self.text_b = text_b\n","    self.text_c = text_c\n","    self.label = label\n","\n","\n","class InputFeatures(object):\n","  def __init__(self, input_ids, input_mask, segment_ids, label_id):\n","    self.input_ids = input_ids\n","    self.input_mask = input_mask\n","    self.segment_ids = segment_ids\n","    self.label_id = label_id\n","\n","\n","class DataProcessor(object):\n","  def get_train_examples(self, data_dir):\n","    raise NotImplementedError()\n","\n","  def get_dev_examples(self, data_dir):\n","    raise NotImplementedError()\n","\n","  def get_labels(self):\n","    raise NotImplementedError()\n","\n","  @classmethod\n","  def _read_tsv(cls, input_file, quotechar=None):\n","    with open(input_file, \"r\") as f:\n","      reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n","      lines = []\n","\n","      for line in reader:\n","        lines.append(line)\n","      \n","      return lines\n","\n","\n","class dreamProcessor(DataProcessor):\n","  def __init__(self, fpath):\n","    self.D = [[], [], []]\n","    self.fpath = fpath\n","\n","    for sid in range(3):\n","      with open([self.fpath+'/train.json', self.fpath+'/dev.json', self.fpath+'/test.json'][sid], \"r\") as f:\n","        data = json.load(f)\n","\n","        for i in range(len(data)):\n","          for j in range(len(data[i][1])):\n","            d = ['\\n'.join(data[i][0]).lower(), data[i][1][j][\"question\"].lower()]\n","\n","            for k in range(len(data[i][1][j][\"choice\"])):\n","              d += [data[i][1][j][\"choice\"][k].lower()]\n","            \n","            d += [data[i][1][j][\"answer\"].lower()] \n","            self.D[sid] += [d]\n","\n","  def get_train_examples(self):\n","    return self._create_examples(self.D[0], \"train\")\n","\n","  def get_test_examples(self):\n","    return self._create_examples(self.D[2], \"test\")\n","\n","  def get_dev_examples(self):\n","    return self._create_examples(self.D[1], \"dev\")\n","\n","  def get_labels(self):\n","    return [\"0\", \"1\", \"2\"]\n","\n","  def _create_examples(self, data, set_type):\n","    examples = []\n","\n","    for (i, d) in enumerate(data):\n","      for k in range(3):\n","        if data[i][2+k] == data[i][5]:\n","          answer = str(k)\n","              \n","      label = tokenization.convert_to_unicode(answer)\n","\n","      for k in range(3):\n","        guid = \"%s-%s-%s\" % (set_type, i, k)\n","        text_a = tokenization.convert_to_unicode(data[i][0])\n","        text_b = tokenization.convert_to_unicode(data[i][k+2])\n","        text_c = tokenization.convert_to_unicode(data[i][1])\n","        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label, text_c=text_c))\n","        \n","    return examples\n","\n","\n","def convert_examples_to_features(examples, label_list, left_len, right_len, tokenizer, n_class):\n","  label_map, features, stat = {}, [[]], []\n","\n","  for (i, label) in enumerate(label_list):\n","    label_map[label] = i\n","\n","  for (ex_index, example) in enumerate(examples):\n","    tokens_a = tokenizer.tokenize(example.text_a)\n","    tokens_b = None\n","    tokens_c = None\n","    \n","    if example.text_b:\n","      tokens_b = tokenizer.tokenize(example.text_b)\n","\n","    if example.text_c:\n","      tokens_c = tokenizer.tokenize(example.text_c)\n","\n","    stat.append([len(tokens_a), len(tokens_b), len(tokens_c)])\n","    tokens_a = tokens_a[0:(left_len-2)]\n","    _truncate_seq_pair(tokens_b, tokens_c, right_len-2)\n","    tokens_b = tokens_c+[\"[SEP]\"]+tokens_b\n","    tokens, segment_ids = [], []\n","    tokens.append(\"[CLS]\")\n","    segment_ids.append(0)\n","\n","    for token in tokens_a:\n","      tokens.append(token)\n","      segment_ids.append(0)\n","\n","    tokens.append(\"[SEP]\")\n","    segment_ids.append(0)\n","    input_mask = [1]*len(tokens)\n","\n","    while len(tokens) < left_len:\n","      tokens.append(\"[PAD]\")\n","      input_mask.append(0)\n","      segment_ids.append(0)\n","\n","    if tokens_b:\n","      for token in tokens_b:\n","        tokens.append(token)\n","        segment_ids.append(1)\n","        input_mask.append(1)\n","      \n","      tokens.append(\"[SEP]\")\n","      segment_ids.append(1)\n","      input_mask.append(1)\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","    while len(input_ids) < left_len+right_len:\n","      input_ids.append(0)\n","      input_mask.append(0)\n","      segment_ids.append(0)\n","\n","    label_id = label_map[example.label]\n","    features[-1].append(InputFeatures(\n","      input_ids=input_ids,\n","      input_mask=input_mask,\n","      segment_ids=segment_ids,\n","      label_id=label_id))\n","\n","    if len(features[-1]) == n_class:\n","      features.append([])\n","\n","  if len(features[-1]) == 0:\n","    features = features[:-1]\n","\n","  return features, stat\n","\n","\n","def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n","  while True:\n","    total_length = len(tokens_a)+len(tokens_b)\n","\n","    if total_length <= max_length:\n","      break\n","    \n","    if len(tokens_a) > len(tokens_b):\n","      tokens_a.pop()\n","    else:\n","      tokens_b.pop()\n","\n","\n","def _truncate_seq_tuple(tokens_a, tokens_b, tokens_c, max_length):\n","  while True:\n","    total_length = len(tokens_a)+len(tokens_b)+len(tokens_c)\n","\n","    if total_length <= max_length:\n","      break\n","    \n","    if len(tokens_a) >= len(tokens_b) and len(tokens_a) >= len(tokens_c):\n","      tokens_a.pop()\n","    \n","    elif len(tokens_b) >= len(tokens_a) and len(tokens_b) >= len(tokens_c):\n","      tokens_b.pop()\n","    else:\n","      tokens_c.pop()            \n","\n","\n","def accuracy(out, labels):\n","  outputs = np.argmax(out, axis=1)\n","  return np.sum(outputs==labels)\n","\n","\n","def data_processing(data, label, leftlen, rightlen, tokenizer, nclass):\n","  train_features, stat = convert_examples_to_features(data, label, leftlen, rightlen, tokenizer, nclass)\n","  input_ids, input_mask, segment_ids, label_id = [], [], [], []\n","\n","  for f in train_features:\n","    input_ids.append([])\n","    input_mask.append([])\n","    segment_ids.append([])\n","\n","    for i in range(nclass):\n","      input_ids[-1].append(f[i].input_ids)\n","      input_mask[-1].append(f[i].input_mask)\n","      segment_ids[-1].append(f[i].segment_ids)\n","    \n","    label_id.append([f[0].label_id])\n","\n","  return input_ids, input_mask, segment_ids, label_id, stat\n","\n","\n","def data_preparing(text, seg, mask, label, batch, training, strategy):\n","  text1, seg1, mask1, label1 = np.array(text), np.array(seg), np.array(mask), np.array(label)\n","  data1 = tf.data.Dataset.from_tensor_slices((text1, seg1, mask1, label1))\n","  data1 = data1.shuffle(len(text1)).batch(batch) if training else data1.batch(batch)\n","  return strategy.experimental_distribute_datasets_from_function(lambda _: data1)\n","\n","\n","batch_1 = BATCH//strategy_1.num_replicas_in_sync\n","processor_1 = dreamProcessor(FILEPATH)\n","label_1 = processor_1.get_labels()\n","tokenizer_1 = tokenization.FullTokenizer.from_scratch(vocab_file=VOCAB, do_lower_case=LOWER, spm_model_file=SPM)\n","training_1 = data_processing(processor_1.get_train_examples(), label_1, LEFTLEN, RIGHTLEN, tokenizer_1, NCLASS)\n","training_2 = data_preparing(training_1[0], training_1[2], training_1[1], training_1[3], batch_1, True, strategy_1)\n","dev_1 = data_processing(processor_1.get_dev_examples(), label_1, LEFTLEN, RIGHTLEN, tokenizer_1, NCLASS)\n","dev_2 = data_preparing(dev_1[0], dev_1[2], dev_1[1], dev_1[3], batch_1, False, strategy_1)\n","test_1 = data_processing(processor_1.get_test_examples(), label_1, LEFTLEN, RIGHTLEN, tokenizer_1, NCLASS)\n","test_2 = data_preparing(test_1[0], test_1[2], test_1[1], test_1[3], batch_1, False, strategy_1)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPHLiOJ041VH"},"source":["Modeling with ALBERT and Co-Attention layer."]},{"cell_type":"code","metadata":{"id":"RZj2lAHIa05b"},"source":["def w_initializing(param=0.02):\n","  return keras.initializers.TruncatedNormal(stddev=param)\n","\n","\n","class AdamW(keras.optimizers.Adam):\n","  def __init__(self, step, lrate=1e-3, drate=1e-2, name='AdamW', **kwargs):\n","    super(AdamW, self).__init__(learning_rate=lrate, name=name, **kwargs)\n","    self.step, self.drate, self.spec = step, drate, ['bias', 'normalization', 'lnorm', 'layernorm']\n","\n","  @staticmethod\n","  def _rate_sch(rate, step, total):\n","    warm1 = total*0.1\n","    return tf.where(step < warm1, rate*step/warm1, rate*(total-step)/(total-warm1))\n","\n","  def _prepare_local(self, var_device, var_dtype, apply_state):\n","    super(AdamW, self)._prepare_local(var_device, var_dtype, apply_state)\n","    rate1 = self._rate_sch(1., tf.cast(self.iterations+1, var_dtype), self.step+1)\n","    apply_state[(var_device, var_dtype)]['lr_t'] *= rate1\n","    apply_state[(var_device, var_dtype)]['lr'] *= rate1\n","\n","  def _resource_apply_base(self, var, apply_state=None):\n","    devi1, type1, spec1 = var.device, var.dtype.base_dtype, any(c1 in var.name.lower() for c1 in self.spec)\n","    coef1 = ((apply_state or {}).get((devi1, type1)) or self._fallback_apply_state(devi1, type1))\n","    return tf.no_op if spec1 else var.assign_sub(coef1['lr_t']*var*self.drate, use_locking=self._use_locking)\n","\n","  def _resource_apply_dense(self, grad, var, apply_state=None):\n","    deca1 = self._resource_apply_base(var, apply_state)\n","\n","    with tf.control_dependencies([deca1]):\n","      return super(AdamW, self)._resource_apply_dense(grad, var, apply_state)\n","\n","  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n","    deca1 = self._resource_apply_base(var, apply_state)\n","\n","    with tf.control_dependencies([deca1]):\n","      return super(AdamW, self)._resource_apply_sparse(grad, var, indices, apply_state)\n","\n","  def get_config(self):\n","    conf1 = super(AdamW, self).get_config()\n","    conf1.update({'decaying_rate': self.drate, 'step': self.step})\n","    return conf1\n","\n","\n","class CoAttention(keras.layers.Layer):\n","  def __init__(self, bname, head, size, attdrop=0., drop=0., eps=1e-6, ninf=-1e4, **kwargs):\n","    super(CoAttention, self).__init__(**kwargs)\n","    self.head, self.size, self.dim, self.ninf = head, size, head*size, ninf\n","    self.wq = keras.layers.Dense(self.dim, None, True, w_initializing(), name=bname+'/attention/query')\n","    self.wk = keras.layers.Dense(self.dim, None, True, w_initializing(), name=bname+'/attention/key')\n","    self.wv = keras.layers.Dense(self.dim, None, True, w_initializing(), name=bname+'/attention/value')\n","    self.dense = keras.layers.Dense(self.dim, None, True, w_initializing(), name=bname+'/attention/dense')\n","    self.norm = keras.layers.LayerNormalization(-1, eps, name=bname+'/attention/LayerNorm')\n","    self.attdrop = keras.layers.Dropout(attdrop)\n","    self.drop = keras.layers.Dropout(drop)\n","\n","  def transposing(self, x):\n","    return tf.transpose(tf.reshape(x, [-1, tf.shape(x)[1], self.head, self.size]), [0, 2, 1, 3])\n","\n","  def masking(self, mask):\n","    m1 = mask[:, tf.newaxis, tf.newaxis, :]\n","    return tf.cast(m1, tf.float32)*self.ninf\n","\n","  def calculating(self, q, k, v, mask, training):\n","    a1 = tf.matmul(self.transposing(q), self.transposing(k), transpose_b=True)\n","    a1 = a1/tf.math.sqrt(tf.cast(self.size, tf.float32))\n","    a1 = tf.nn.softmax(a1+self.masking(1-mask) if mask is not None else a1, axis=-1)\n","    return tf.matmul(self.attdrop(a1, training=training), self.transposing(v)), a1\n","\n","  def propagating(self, left, right, mask=None, training=False):\n","    x1, a1 = self.calculating(self.wq(left), self.wk(right), self.wv(right), mask, training)\n","    x1 = tf.reshape(tf.transpose(x1, [0, 2, 1, 3]), [-1, tf.shape(x1)[2], self.dim])\n","    return self.norm(left+self.drop(self.dense(x1), training=training))\n","\n","\n","class ModelBERT(keras.Model):\n","  def __init__(self, model, head, size, drop, nclass, leftlen, rightlen, klayer, indim=768):\n","    super(ModelBERT, self).__init__()\n","    self.nclass, self.size, self.left, self.klayer = nclass, [-1, leftlen+rightlen], leftlen, klayer\n","    self.bert = TFAlbertModel.from_pretrained(model)\n","    self.outdense = keras.layers.Dense(head*size) if head*size != indim else None\n","    self.coa = CoAttention('bert/coattention', head, size, drop, drop)\n","    self.poola = keras.layers.GlobalAveragePooling1D()\n","    self.poolb = keras.layers.GlobalAveragePooling1D()\n","    self.drop = keras.layers.Dropout(drop)\n","    self.dense = keras.layers.Dense(1)\n","\n","  def propagating(self, text, seg, mask, training):\n","    x1, x2, x3 = tf.reshape(text, self.size), tf.reshape(seg, self.size), tf.reshape(mask, self.size)\n","    x1 = self.bert(input_ids=x1, attention_mask=x3, token_type_ids=x2, training=training)['last_hidden_state']\n","\n","    if self.outdense is not None:\n","      x1 = self.outdense(x1)\n","\n","    stat1, stat2 = x1[:, :self.left], x1[:, self.left:]\n","    mask1, mask2 = x3[:, :self.left], x3[:, self.left:]\n","\n","    for i1 in range(self.klayer):\n","      stat1 = self.coa.propagating(stat1, stat2, mask2, training)\n","      stat2 = self.coa.propagating(stat2, stat1, mask1, training)\n","\n","    stat1 = stat1*tf.cast(mask1, tf.float32)[:, :, tf.newaxis]\n","    stat2 = stat2*tf.cast(mask2, tf.float32)[:, :, tf.newaxis]\n","    stat1 = tf.concat([self.poola(stat1), self.poolb(stat2)], 1)\n","    stat1 = self.dense(self.drop(stat1, training=training))\n","    return tf.nn.softmax(tf.reshape(stat1, [-1, self.nclass]))\n","\n","\n","@tf.function\n","def step_training(iterator):\n","  def training(data):\n","    text_1, seg_1, mask_1, label_1 = data\n","\n","    with tf.GradientTape() as tape_1:\n","      pred_1 = model_1.propagating(text_1, seg_1, mask_1, True)\n","      value_1 = function_1(label_1, pred_1)\n","      value_1 = tf.nn.compute_average_loss(value_1, global_batch_size=BATCH)\n","\n","    grad_1 = tape_1.gradient(value_1, model_1.trainable_variables)\n","    grad_1, _ = tf.clip_by_global_norm(grad_1, 1.0)\n","    optimizer_1.apply_gradients(list(zip(grad_1, model_1.trainable_variables)))\n","    loss_1.update_state(value_1*strategy_1.num_replicas_in_sync)\n","    acc_1.update_state(label_1, pred_1)\n","    \n","  strategy_1.run(training, args=(next(iterator),))\n","\n","\n","@tf.function\n","def step_evaluating(iterator):\n","  def evaluating(data):\n","    text_1, seg_1, mask_1, label_1 = data\n","    pred_1 = model_1.propagating(text_1, seg_1, mask_1, False)\n","    acc_2.update_state(label_1, pred_1)\n","\n","  strategy_1.run(evaluating, args=(next(iterator),))\n","\n","\n","with strategy_1.scope():\n","  model_1 = ModelBERT(MODEL, HEAD, SIZE, DROP, NCLASS, LEFTLEN, RIGHTLEN, KLAYER, INDIM)\n","  function_1 = keras.losses.SparseCategoricalCrossentropy(reduction=keras.losses.Reduction.NONE)\n","  optimizer_1 = AdamW(EPOCH*(int(len(training_1[0])/BATCH)+1), LRATE)\n","  loss_1 = keras.metrics.Mean(name='training_loss')\n","  acc_1 = keras.metrics.SparseCategoricalAccuracy(name='training_accuracy')\n","  acc_2 = keras.metrics.SparseCategoricalAccuracy(name='dev_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-kZtSX8ecC4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617601565337,"user_tz":-480,"elapsed":268805,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"85c285df-6b80-4140-a71a-3aa637b86a31"},"source":["print_1 = 'Training loss is {:.4f}, and accuracy is {:.4f}.'\n","print_2 = 'Dev accuracy is {:.4f}, and epoch cost is {:.4f}.'\n","\n","for e_1 in range(EPOCH):\n","  print('Epoch {} running.'.format(e_1+1))\n","  time_0, training_3, dev_3 = time.time(), iter(training_2), iter(dev_2)\n","\n","  for s_1 in range(math.floor(len(training_1[0])/BATCH)):\n","    step_training(training_3)\n","\n","    if (s_1+1) % 50 == 0:\n","      print(print_1.format(float(loss_1.result()), float(acc_1.result())))\n","\n","  for s_1 in range(math.ceil(len(dev_1[0])/BATCH)):\n","    step_evaluating(dev_3)\n","\n","  print(print_2.format(float(acc_2.result()), time.time()-time_0))\n","  print('**********')\n","  acc_1.reset_states()\n","  acc_2.reset_states()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Epoch 1 running.\n","Training loss is 1.0971, and accuracy is 0.3450.\n","Training loss is 1.0748, and accuracy is 0.4106.\n","Training loss is 1.0479, and accuracy is 0.4437.\n","Training loss is 1.0200, and accuracy is 0.4756.\n","Training loss is 1.0058, and accuracy is 0.4970.\n","Training loss is 0.9856, and accuracy is 0.5131.\n","Training loss is 0.9727, and accuracy is 0.5334.\n","Dev accuracy is 0.6299, and epoch cost is 132.9115.\n","**********\n","Epoch 2 running.\n","Training loss is 0.9395, and accuracy is 0.7050.\n","Training loss is 0.9179, and accuracy is 0.7244.\n","Training loss is 0.9044, and accuracy is 0.7154.\n","Training loss is 0.8944, and accuracy is 0.7175.\n","Training loss is 0.8840, and accuracy is 0.7207.\n","Training loss is 0.8742, and accuracy is 0.7225.\n","Training loss is 0.8668, and accuracy is 0.7223.\n","Dev accuracy is 0.6647, and epoch cost is 67.4803.\n","**********\n","Epoch 3 running.\n","Training loss is 0.8440, and accuracy is 0.8338.\n","Training loss is 0.8254, and accuracy is 0.8275.\n","Training loss is 0.8055, and accuracy is 0.8304.\n","Training loss is 0.7886, and accuracy is 0.8322.\n","Training loss is 0.7740, and accuracy is 0.8295.\n","Training loss is 0.7613, and accuracy is 0.8290.\n","Training loss is 0.7515, and accuracy is 0.8289.\n","Dev accuracy is 0.6701, and epoch cost is 67.5821.\n","**********\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bVfczI_8ObtX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617601588670,"user_tz":-480,"elapsed":7636,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"96b9338b-1f4a-4a12-c29b-8cca2ab82ea8"},"source":["test_3 = iter(test_2)\n","\n","for s_1 in range(math.ceil(len(test_1[0])/BATCH)):\n","  step_evaluating(test_3)\n","\n","print('Test accuracy is {:.4f}.'.format(float(acc_2.result())))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Test accuracy is 0.6712.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xPKS4WzOO6l9"},"source":["Have a test!"]},{"cell_type":"code","metadata":{"id":"XT45BL5GO6Ko","executionInfo":{"status":"ok","timestamp":1617601602559,"user_tz":-480,"elapsed":738,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}}},"source":["class MRCBot(object):\n","  def __init__(self, processor, tokenizer, model):\n","    self.processor, self.tokenizer, self.model = processor, tokenizer, model\n","\n","  def processing(self, examples):\n","    pair1 = []\n","\n","    for i1, text1 in enumerate(examples):\n","      cont1 = '\\n'.join(text1['passage'])\n","      ques1 = text1['question']\n","      cont1 = cont1.lower() if LOWER else cont1\n","      ques1 = ques1.lower() if LOWER else ques1\n","      \n","      for j1, answ1 in enumerate(text1['options']):\n","        answ1 = answ1.lower() if LOWER else answ1\n","        pair1.append(InputExample(guid=0, text_a=cont1, text_b=answ1, label='0', text_c=ques1))\n","\n","    data1 = data_processing(pair1, self.processor.get_labels(), LEFTLEN, RIGHTLEN, self.tokenizer, NCLASS)\n","    return np.array(data1[0]), np.array(data1[2]), np.array(data1[1])\n","\n","  def predicting(self, examples):\n","    x1, x2, x3 = self.processing(examples)\n","    pred1 = np.argmax(self.model.propagating(x1, x2, x3, False), -1).tolist()\n","    return [examples[i1]['options'][pred1[i1]] for i1 in range(len(examples))]\n","\n","\n","bot_1 = MRCBot(processor_1, tokenizer_1, model_1)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"fXjhDmNPO6Bx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617601607654,"user_tz":-480,"elapsed":2488,"user":{"displayName":"Chris Chen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib6pz72VKmdLMqZPhKb2UKgIAW_6HhaEfhtM5C=s64","userId":"15295307384944031263"}},"outputId":"3995e96d-2735-48bc-d6de-dc8d0f7d3440"},"source":["sample_1 = [{\n","  'passage': [\n","    'm: my name is andy tao, very nice to meet you.',\n","    'w: nice to meet you, my name is jennifer, and thank you for this expensive dinner.',\n","    'm: you are welcome. you know, i am quite rich, so it is a piece of cake.',\n","    'w: you are so humorous, i am looking forward to our next dating.',\n","    'm: me too. I can drive my luxurious tesla and take you to a nice park.'],\n","  'question': 'what is the man going to do for next dating?',\n","  'options': [\n","    'drive the woman to a park.',\n","    'show the woman his luxurious tesla.',\n","    'take the woman to a restaurant for dinner.']}]\n","\n","bot_1.predicting(sample_1)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['drive the woman to a park.']"]},"metadata":{"tags":[]},"execution_count":8}]}]}